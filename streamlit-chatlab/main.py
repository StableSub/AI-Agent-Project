import os, sys, asyncio, hashlib, tempfile
import streamlit as st

from llm_model import load_model
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_mcp_adapters.prompts import load_mcp_prompt
from langgraph.prebuilt import create_react_agent
from langchain.tools.retriever import create_retriever_tool
import pandas as pd

from RAG import rag_test
from EDA import run_eda

os.environ["TOKENIZERS_PARALLELISM"] = "false"

st.set_page_config(page_title="Sub's Agent", page_icon="ğŸ˜€")
st.title("My_Little_LLM")
st.caption("ë­ì²´ì¸ ë° ìŠ¤íŠ¸ë¦¼ë¦¿ í…ŒìŠ¤íŠ¸")

with st.sidebar:
    "[ê¹ƒí—ˆë¸Œ](https://github.com/StableSub)"

LLM = load_model.load_llm()

SERVER_PARAMS = StdioServerParameters(
    command=sys.executable,
    args=["-u", os.path.abspath("MCP/server.py")],
)

# ì—…ë¡œë“œ ëœ íŒŒì¼ì˜ ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ í•´ì‹œë¡œ ê³„ì‚°.
# ê°™ì€ íŒŒì¼ì´ ì—¬ëŸ¬ ë²ˆ ì—…ë¡œë“œ ë˜ì–´ë„ ì¤‘ë³µ ì¸ë±ì‹±ì„ ë°©ì§€.
def file_md5(uploaded_file) -> str | None:
    if not uploaded_file:
        return None
    return hashlib.md5(uploaded_file.getvalue()).hexdigest()

# Streamlitì˜ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”.
# ì‚¬ìš©ìì™€ LLM ê°„ì˜ ëŒ€í™” ë‚´ìš©, ì—…ë¡œë“œ íŒŒì¼ ì˜ˆì‹œ ë° retriever ê°ì²´ ë“±ì„ ì´ˆê¸°ê°’ìœ¼ë¡œ ì„¤ì •.
def init_state():
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "íŒŒì¼ ì—…ë¡œë“œ í›„ ì§ˆë¬¸í•´ ë³´ì„¸ìš”!"}]
    if "file_hash" not in st.session_state:
        st.session_state.file_hash = None
    if "retriever" not in st.session_state:
        st.session_state.retriever = None
    if "agent_tool_count" not in st.session_state:
        st.session_state.agent_tool_count = 0

init_state()

uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=("pdf", "csv", "xlsx", "json",))
if uploaded_file:
    message = f"{uploaded_file.name}ì— ëŒ€í•´ ì§ˆë¬¸ í•´ë³´ì„¸ìš”"
    st.session_state.messages = [{"role": "assistant", "content": message}]
new_hash = file_md5(uploaded_file)

# ì—…ë¡œë“œëœ íŒŒì¼ì´ ì´ì „ì— ì—†ë˜ íŒŒì¼ì´ë¼ë©´ ì¡°ê±´ë¬¸ ì§„ì….
if uploaded_file and new_hash != st.session_state.file_hash:
    st.info("ìƒˆ íŒŒì¼ ê°ì§€ â†’ ì¸ë±ì‹± ì¤‘...")
    st.session_state.retriever = rag_test(uploaded_file, LLM) # RAGìš© retriever ê°ì²´ ìƒì„±.
    st.session_state.file_hash = new_hash
    suffix = "." + uploaded_file.name.split(".")[-1].lower() # ì—…ë¡œë“œëœ ë¬¸ì„œì˜ í™•ì¥ì ì¶”ì¶œ.
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp: # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì„ì‹œ ê²½ë¡œë¡œ ì €ì¥.
        tmp.write(uploaded_file.getvalue())
        tmp_path = tmp.name
        st.session_state["uploaded_path"] = tmp_path
        st.success(f"íŒŒì¼ ì €ì¥ ì™„ë£Œ: {tmp_path}")
    df = pd.read_csv(tmp_path)
    run_eda(df)

# ì—¬íƒœê¹Œì§€ ì €ì¥ëœ ë©”ì„¸ì§€ ì¶œë ¥.
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ MCP Agentë¥¼ ì‹¤í–‰í•˜ê³ , ì ì ˆí•œ íˆ´ì„ ì‚¬ìš©í•´ ì‘ë‹µ ìƒì„±.
async def run(user_input: str):
    """ë§¤ ì…ë ¥ë§ˆë‹¤ 1íšŒ í˜¸ì¶œ: MCP ì—°ê²° â†’ íˆ´ êµ¬ì„± â†’ ì—ì´ì „íŠ¸ ìƒì„±/í˜¸ì¶œ."""
    async with stdio_client(SERVER_PARAMS) as (read, write): # MCP ì„œë²„ì— ì—°ê²°.
        async with ClientSession(read, write) as session: # íˆ´ ë° ë¥´í¼í”„íŠ¸ ë¡œë”©ì„ ìœ„í•´ ì´ˆê¸°í™” ì‹¤í–‰.
            await session.initialize()
            
            # MCPì— ë“±ë¡ëœ íˆ´ë“¤ì„ LangChain Agentê°€ ì‚¬ìš©í•  ìˆ˜ ìˆë˜ë¡ ë¦¬ìŠ¤íŠ¸ë¡œ ë¡œë“œ
            mcp_tools = await load_mcp_tools(session)
            tools = list(mcp_tools)

            # ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ìˆìœ¼ë©´, í•´ë‹¹ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” retrieverë¥¼ LangChain íˆ´ë¡œ ë“±ë¡.
            if st.session_state.retriever:
                retriever_tool = create_retriever_tool(
                    st.session_state.retriever,
                    # toll ì´ë¦„ ë° Agentê°€ ì–´ë–¤ ì§ˆë¬¸ì— ì´ íˆ´ì„ ì‚¬ìš©í• ì§€ë¥¼ ê²°ì •.
                    name="doc_search",
                    description=(
                        "ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ìŠµë‹ˆë‹¤. "
                        "ìš”ì•½/ê°œìš”/í•µì‹¬ ì •ë¦¬ ë“±ë„ ì´ ë„êµ¬ë¡œ í•„ìš”í•œ ê·¼ê±°ë¥¼ ìˆ˜ì§‘í•œ í›„ ì‘ì„±í•˜ì„¸ìš”."
                    ),
                )
                tools.append(retriever_tool)
                if st.session_state.retriever:
                    st.success("retriever ìƒì„± ì™„ë£Œ")
                else:
                    st.warning("retriever ìƒì„± ì‹¤íŒ¨")

            # LLMì´ ììœ ë¡­ê²Œ íˆ´ì„ ì„ íƒ.
            agent = create_react_agent(LLM, tools)

            # default_promptë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ì‚¬ìš©í•  ì´ˆê¸° ë©”ì„¸ì§€ ìƒì„±.
            prompts = await load_mcp_prompt(session, "default_prompt", arguments={"message": user_input})
            
            # LLM Agentë¥¼ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µ ìƒì„±.
            response = await agent.ainvoke({"messages": prompts})
            answer = response["messages"][-1].content
            return answer
        
if user_input := st.chat_input("Say something"):
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content" : user_input})
    with st.chat_message("assistant"):
        thinking_msg = st.empty()
        thinking_msg.markdown("ğŸ’­ ìƒê° ì¤‘ì…ë‹ˆë‹¤...")
        try:
            answer = asyncio.run(run(user_input))
        except Exception as e:
            answer = f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        thinking_msg.empty()
        st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})